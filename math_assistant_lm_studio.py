import json
import random
import requests

# è¨­å®š LM Studio API åƒæ•¸
LM_API_URL = "http://localhost:1234/v1/chat/completions"  # ç¢ºä¿ LM Studio æ­£åœ¨é‹è¡Œ
MODEL_NAME = "mistral-nemo-instruct-2407"  # è«‹æ›´æ›ç‚ºä½ çš„æ¨¡å‹åç¨±ï¼ˆå¦‚ 'mistral-7b.Q4_K_M.gguf'ï¼‰

# è®€å–æ¨™æº– JSON æ ¼å¼çš„ Ape210K é¡Œåº«
with open("ape210k_test.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# éš¨æ©Ÿé¸æ“‡ä¸€é“é¡Œç›®
def get_random_question(data):
    question = random.choice(data)
    return {
        "question": question["original_text"],
        "answer": question["ans"],
        "equation": question["equation"]
    }

# å‘¼å« LM Studio API è®“ LLM åˆ¤æ–·ç­”æ¡ˆæ˜¯å¦æ­£ç¢ºï¼ˆæ”¯æ´æµå¼è¼¸å‡ºï¼‰
def check_answer_with_llm(question, correct_answer, user_answer, chat_history):
    prompt = f"""
    é€™æ˜¯ä¸€é“å°å­¸æ•¸å­¸é¡Œç›®ï¼š
    å•é¡Œï¼š{question}

    å­¸ç”Ÿè¼¸å…¥çš„ç­”æ¡ˆæ˜¯ï¼š{user_answer}
    æ­£ç¢ºç­”æ¡ˆæ‡‰è©²æ˜¯ï¼š{correct_answer}

    ä½ çš„ä»»å‹™ï¼š
    1. åˆ¤æ–·å­¸ç”Ÿçš„ç­”æ¡ˆæ˜¯å¦èˆ‡æ­£ç¢ºç­”æ¡ˆç›¸åŒï¼Œå…è¨±ä¸åŒçš„å–®ä½ï¼ˆå¦‚ã€Œ1.4ç±³ã€å’Œã€Œ1.4ã€ï¼‰ã€‚
    2. å¿½ç•¥ç­”æ¡ˆä¸­æ–‡èˆ‡æ•¸å­—çš„å·®ç•°ï¼Œå¦‚5èˆ‡äº”ä»½æ‡‰è¦–ç‚ºç›¸åŒã€‚
    3. è‹¥ç­”æ¡ˆæ­£ç¢ºï¼Œè«‹å›æ‡‰ï¼šã€Œâœ… ä½ çš„ç­”æ¡ˆæ­£ç¢ºï¼ã€ã€‚
    4. è‹¥ç­”æ¡ˆéŒ¯èª¤ï¼Œè«‹å›æ‡‰ï¼šã€ŒâŒ ä½ çš„ç­”æ¡ˆéŒ¯èª¤ï¼Œæ­£ç¢ºç­”æ¡ˆæ˜¯ {correct_answer}ã€‚ã€ï¼Œä¸¦è§£é‡‹è©²é¡Œæ‡‰å¦‚ä½•è§£é¡Œã€‚
    """

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°å­¸æ•¸å­¸è€å¸«ï¼Œè² è²¬æ‰¹æ”¹å­¸ç”Ÿçš„ç­”æ¡ˆä¸¦æä¾›è§£é‡‹ã€‚"},
        {"role": "user", "content": prompt}
    ]
    messages.extend(chat_history)  # âœ… ä¿æŒä¸Šä¸‹æ–‡

    payload = {
        "model": MODEL_NAME,
        "messages": messages,
        "temperature": 0.2,
        "stream": True  # âœ… å•Ÿç”¨æµå¼è¼¸å‡º
    }

    with requests.post(LM_API_URL, json=payload, stream=True) as response:
        print("\nğŸ“– LLM åˆ¤æ–·çµæœï¼š", end="", flush=True)
        llm_response = ""

        for line in response.iter_lines():
            if line:
                try:
                    line = line.decode("utf-8").replace("data: ", "").strip()
                    data = json.loads(line)
                    content = data["choices"][0]["delta"].get("content", "")
                    print(content, end="", flush=True)
                    llm_response += content
                except json.JSONDecodeError:
                    continue

        print("\n")
    
    # âœ… å°‡é€™æ¬¡çš„å°è©±è¨˜éŒ„åŠ å…¥ chat_history
    chat_history.append({"role": "user", "content": user_answer})
    chat_history.append({"role": "assistant", "content": llm_response})

# ä¸»ç¨‹å¼
if __name__ == "__main__":
    print("\nğŸ“ æ­¡è¿ä¾†åˆ°å°å­¸ç”Ÿæ•¸å­¸è¼”åŠ©ç³»çµ±ï¼")
    print("ğŸ”¹ ä½ å¯ä»¥è¼¸å…¥ä½ çš„ç­”æ¡ˆï¼ŒLLM æœƒå¹«ä½ æª¢æŸ¥æ­£ç¢ºæ€§ã€‚")
    print("ğŸ”¹ å¦‚æœæœ‰å…¶ä»–å•é¡Œï¼Œä¹Ÿå¯ä»¥ç¹¼çºŒæå•ï¼")
    print("ğŸ”¹ æŒ‰ `Ctrl+C` æˆ– `Ctrl+D` é€€å‡ºç¨‹å¼ã€‚\n")

    try:
        while True:  # âœ… å…è¨±å¤šè¼ªå°è©±
            sample_question = get_random_question(data)
            print("\nğŸ“Œ é¡Œç›®:", sample_question["question"])

            chat_history = []  # âœ… æ¯é“é¡Œç›®é‡ç½®æ­·å²ç´€éŒ„

            # è®“ä½¿ç”¨è€…è¼¸å…¥ç­”æ¡ˆ
            user_answer = input("âœï¸ è«‹è¼¸å…¥ä½ çš„ç­”æ¡ˆï¼š ")

            # è®“ LLM å¹«åŠ©åˆ¤æ–·ç­”æ¡ˆæ˜¯å¦æ­£ç¢ºï¼ˆæµå¼è¼¸å‡ºï¼‰
            check_answer_with_llm(
                sample_question["question"],
                sample_question["answer"],
                user_answer,
                chat_history
            )

            # å…è¨±ä½¿ç”¨è€…ç¹¼çºŒæå•ï¼Œä¸¦ä¿æŒä¸Šä¸‹æ–‡
            while True:
                follow_up = input("\nğŸ’¡ ä½ é‚„æœ‰å…¶ä»–å•é¡Œå—ï¼Ÿ(è¼¸å…¥å•é¡Œæˆ–æŒ‰ Enter è®“æˆ‘å‡ºæ–°é¡Œç›®)ï¼š ").strip()
                if follow_up == "":
                    break  # è®“ç³»çµ±å‡ºä¸‹ä¸€é“é¡Œç›®
                else:
                    # âœ… å°‡å°è©±ä¸Šä¸‹æ–‡åŠ å› LLMï¼Œä¿æŒé€£è²«æ€§
                    chat_history.append({"role": "user", "content": follow_up})

                    payload = {
                        "model": MODEL_NAME,
                        "messages": [
                            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°å­¸æ•¸å­¸è€å¸«ï¼Œå°ˆé–€å¹«åŠ©å­¸ç”Ÿå­¸ç¿’ã€‚"}
                        ] + chat_history,
                        "temperature": 0.7,
                        "stream": True  # âœ… å•Ÿç”¨æµå¼è¼¸å‡º
                    }

                    with requests.post(LM_API_URL, json=payload, stream=True) as response:
                        print("\nğŸ“– LLM å›ç­”ï¼š", end="", flush=True)
                        llm_response = ""

                        for line in response.iter_lines():
                            if line:
                                try:
                                    line = line.decode("utf-8").replace("data: ", "").strip()
                                    data = json.loads(line)
                                    content = data["choices"][0]["delta"].get("content", "")
                                    print(content, end="", flush=True)
                                    llm_response += content
                                except json.JSONDecodeError:
                                    continue

                        print("\n")

                    # âœ… æŠŠ LLM çš„å›ç­”åŠ å› chat_history
                    chat_history.append({"role": "assistant", "content": llm_response})

    except (KeyboardInterrupt, EOFError):
        print("\nğŸ“š æ„Ÿè¬ä½¿ç”¨å­¸ç¿’ç³»çµ±ï¼Œå†è¦‹ï¼ğŸ‘‹")
